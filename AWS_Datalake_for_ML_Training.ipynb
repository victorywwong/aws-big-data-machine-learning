{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AWS Datalake for ML Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMO9h8uBC+4t97+sdux8zR9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorywwong/aws-big-data-machine-learning/blob/master/AWS_Datalake_for_ML_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnqWNG4Ux1zV",
        "colab_type": "text"
      },
      "source": [
        "AWS Datalake for ML Application"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4EUshy3xxm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import boto3\n",
        "import botocore\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "import project_path # path to helper methods\n",
        "from lib import workshop\n",
        "from pandas import read_sql\n",
        "\n",
        "glue = boto3.client('glue')\n",
        "s3 = boto3.resource('s3')\n",
        "s3_client = boto3.client('s3')\n",
        "cfn = boto3.client('cloudformation')\n",
        "redshift_client = boto3.client('redshift')\n",
        "ec2_client = boto3.client('ec2')\n",
        "rds = boto3.client('rds')\n",
        "\n",
        "session = boto3.session.Session()\n",
        "region = session.region_name\n",
        "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
        "\n",
        "workshop_user = 'db'\n",
        "database_name = 'training' # AWS Glue Data Catalog Database Name\n",
        "environment_name = 'workshop{0}'.format(workshop_user)\n",
        "\n",
        "use_existing = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQNLFFLvzOGy",
        "colab_type": "text"
      },
      "source": [
        "Create S3 Bucket"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5XfhTjyzQmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bucket = workshop.create_bucket(region, session, 'db-')\n",
        "print(bucket)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJqj8Dt6zgEm",
        "colab_type": "text"
      },
      "source": [
        "Create VPC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2S0uxatzhVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if use_existing:\n",
        "    vpc_filter = [{'Name':'isDefault', 'Values':['true']}]\n",
        "    default_vpc = ec2_client.describe_vpcs(Filters=vpc_filter)\n",
        "    vpc_id = default_vpc['Vpcs'][0]['VpcId']\n",
        "\n",
        "    subnet_filter = [{'Name':'vpc-id', 'Values':[vpc_id]}]\n",
        "    subnets = ec2_client.describe_subnets(Filters=subnet_filter)\n",
        "    subnet1_id = subnets['Subnets'][0]['SubnetId']\n",
        "    subnet2_id = subnets['Subnets'][1]['SubnetId']\n",
        "else: \n",
        "    vpc, subnet1, subnet2 = workshop.create_and_configure_vpc()\n",
        "    vpc_id = vpc.id\n",
        "    subnet1_id = subnet1.id\n",
        "    subnet2_id = subnet2.id\n",
        "print(vpc_id)\n",
        "print(subnet1_id)\n",
        "print(subnet2_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdbzOUqKzlO9",
        "colab_type": "text"
      },
      "source": [
        "Install CloudFormation Template"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF9iQGILznCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rds_file = 'sqlserver-rds.yaml'\n",
        "file_path = os.path.join('cfn', rds_file)\n",
        "session.resource('s3').Bucket(bucket).Object(file_path).upload_file(file_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9npzrKjz7AG",
        "colab_type": "text"
      },
      "source": [
        "Execute Script to Install RDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTqUVYh7z_bz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "admin_user = getpass.getpass()\n",
        "admin_password = getpass.getpass()\n",
        "import re\n",
        "\n",
        "pattern = re.compile(r\"^(?=.*[a-z])(?=.*[A-Z])(?=.*\\d)[a-zA-Z\\d]{8,}$\")\n",
        "result = pattern.match(admin_password)\n",
        "if result:\n",
        "    print('Valid')\n",
        "else:\n",
        "    print('Invalid, Password must be 8 characters long alphanumeric only 1 Upper, 1 Lower')\n",
        "\n",
        "cfn_template = 'https://s3.amazonaws.com/{0}/cfn/{1}'.format(bucket, rds_file)\n",
        "print(cfn_template)\n",
        "\n",
        "rds_stack_name = 'SQLServerRDSStack'\n",
        "response = cfn.create_stack(\n",
        "    StackName=rds_stack_name,\n",
        "    TemplateURL=cfn_template,\n",
        "    Capabilities = [\"CAPABILITY_NAMED_IAM\"],\n",
        "    Parameters=[\n",
        "        {\n",
        "            'ParameterKey': 'SqlServerInstanceName',\n",
        "            'ParameterValue': environment_name\n",
        "        },\n",
        "        {\n",
        "            'ParameterKey': 'DatabaseUsername',\n",
        "            'ParameterValue': admin_user\n",
        "        },\n",
        "        {\n",
        "            'ParameterKey': 'DatabasePassword',\n",
        "            'ParameterValue': admin_password\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onkhMZRV0OuQ",
        "colab_type": "text"
      },
      "source": [
        "RDS Status Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_sQNTDV0RfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "response = cfn.describe_stacks(\n",
        "    StackName=rds_stack_name\n",
        ")\n",
        "\n",
        "while response['Stacks'][0]['StackStatus'] != 'CREATE_COMPLETE':\n",
        "    print('Not yet complete.')\n",
        "    time.sleep(30)\n",
        "    response = cfn.describe_stacks(\n",
        "        StackName=rds_stack_name\n",
        "    )\n",
        "\n",
        "for output in response['Stacks'][0]['Outputs']:\n",
        "    if (output['OutputKey'] == 'SQLDatabaseEndpoint'):\n",
        "        rds_endpoint = output['OutputValue']\n",
        "        print('RDS Endpoint: {0}'.format(rds_endpoint))\n",
        "    if (output['OutputKey'] == 'SQLServerSecurityGroup'):\n",
        "        rds_sg_id = output['OutputValue']\n",
        "        print('RDS Security Group: {0}'.format(rds_sg_id))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5YsQHdb3UGG",
        "colab_type": "text"
      },
      "source": [
        "Create DB for labelled data colelction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaV9iNQ23cT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pymssql"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Lf9798E3ccv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pymssql\n",
        "\n",
        "conn = pymssql.connect(rds_endpoint, admin_user, admin_password)\n",
        "conn.autocommit(True)\n",
        "cr=conn.cursor()\n",
        "cr.execute('create database training;')\n",
        "conn.commit()\n",
        "\n",
        "def run_sql_file(filename, connection):\n",
        "    '''\n",
        "    The function takes a filename and a connection as input\n",
        "    and will run the SQL query on the given connection  \n",
        "    '''\n",
        "    file = open(filename, 'r')\n",
        "    sql = s = \" \".join(file.readlines())\n",
        "    cursor = connection.cursor()\n",
        "    cursor.execute(sql)    \n",
        "    connection.commit()\n",
        "  \n",
        "run_sql_file('db-scripts/data-model.sql', conn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyB5ihyB4uQ1",
        "colab_type": "text"
      },
      "source": [
        "Collect Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG12yLni4wlw",
        "colab_type": "text"
      },
      "source": [
        "Get execution role"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWIB8msA4tve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import SageMaker Python SDK to get the Session and execution_role\n",
        "import sagemaker\n",
        "from sagemaker import get_execution_role\n",
        "sagemaker_session = sagemaker.Session()\n",
        "role = get_execution_role()\n",
        "role_name = role[role.rfind('/') + 1:]\n",
        "print(role_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohyvEE4Y4_jb",
        "colab_type": "text"
      },
      "source": [
        "Setup IAM Role\n",
        "\n",
        "\n",
        "```\n",
        "{\n",
        "  \"Version\": \"2012-10-17\",\n",
        "  \"Statement\": [\n",
        "    {\n",
        "      \"Effect\": \"Allow\",\n",
        "      \"Principal\": {\n",
        "        \"Service\": [\n",
        "          \"sagemaker.amazonaws.com\",\n",
        "          \"glue.amazonaws.com\"\n",
        "        ]\n",
        "      },\n",
        "      \"Action\": \"sts:AssumeRole\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3WrNRhU5ytN",
        "colab_type": "text"
      },
      "source": [
        "Get RDS Instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pkiGzHI5SJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "response = rds.describe_db_instances(DBInstanceIdentifier=environment_name)\n",
        "\n",
        "rds_sg_id = response['DBInstances'][0]['VpcSecurityGroups'][0]['VpcSecurityGroupId']\n",
        "print(rds_sg_id)\n",
        "\n",
        "rds_az = response['DBInstances'][0]['AvailabilityZone']\n",
        "print(rds_az)\n",
        "\n",
        "for sub in response['DBInstances'][0]['DBSubnetGroup']['Subnets']:\n",
        "    if sub['SubnetAvailabilityZone']['Name'] == rds_az:\n",
        "        rds_subnet = sub['SubnetIdentifier']\n",
        "        print(rds_subnet)\n",
        "workshop.create_db(glue, account_id, database_name, 'Training data in MS SQL Server database')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9YgGtJM7-Lk",
        "colab_type": "text"
      },
      "source": [
        "Create Glue SQL Connection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B6oiCnW79ED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "db_connection_name = 'MSSQLTrainingConnection{0}'.format(workshop_user)\n",
        "\n",
        "response = glue.create_connection(\n",
        "    CatalogId=account_id,\n",
        "    ConnectionInput={\n",
        "        'Name': db_connection_name,\n",
        "        'Description': 'MS SQL Server Training Connection',\n",
        "        'ConnectionType': 'JDBC',\n",
        "        'MatchCriteria': [\n",
        "            'string',\n",
        "        ],\n",
        "        'ConnectionProperties': {\n",
        "            'JDBC_CONNECTION_URL': 'jdbc:sqlserver://{0};databaseName={1}'.format(rds_endpoint, database_name),\n",
        "            'JDBC_ENFORCE_SSL': 'false',\n",
        "            'PASSWORD': admin_password,\n",
        "            'USERNAME': admin_user\n",
        "        },\n",
        "        'PhysicalConnectionRequirements': {\n",
        "            'SubnetId': rds_subnet,\n",
        "            'SecurityGroupIdList': [\n",
        "                rds_sg_id,\n",
        "            ],\n",
        "            'AvailabilityZone': rds_az\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqcVtFUy8VhG",
        "colab_type": "text"
      },
      "source": [
        "Use Glue Crawler to discover data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBaZ2-bx8cZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "crawler_name = 'MSSQL-Training-Crawler'\n",
        "\n",
        "response = glue.create_crawler(\n",
        "    Name=crawler_name,\n",
        "    Role=role,\n",
        "    DatabaseName=database_name,\n",
        "    Description='Crawler for SQL Server Training Database',\n",
        "    Targets={\n",
        "        'JdbcTargets': [\n",
        "            {\n",
        "                'ConnectionName': db_connection_name,\n",
        "                'Path': database_name\n",
        "            },\n",
        "        ]\n",
        "    },\n",
        "    TablePrefix='R_',\n",
        "    SchemaChangePolicy={\n",
        "        'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
        "        'DeleteBehavior': 'DEPRECATE_IN_DATABASE'\n",
        "    }\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nabCQF1d8-bR",
        "colab_type": "text"
      },
      "source": [
        "Start Glue Crawler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtnZENRf8csn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "response = glue.start_crawler(\n",
        "    Name=crawler_name\n",
        ")\n",
        "\n",
        "print (\"Crawler: https://{0}.console.aws.amazon.com/glue/home?region={0}#crawler:name={1}\".format(region, crawler_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Iz2yw9X9A7V",
        "colab_type": "text"
      },
      "source": [
        "Get Glue Crawler Status"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ1G_p669IkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "crawler_status = glue.get_crawler(Name=crawler_name)['Crawler']['State']\n",
        "while crawler_status not in ('READY'):\n",
        "    crawler_status = glue.get_crawler(Name=crawler_name)['Crawler']['State']\n",
        "    print(crawler_status)\n",
        "    time.sleep(30)\n",
        "print('https://{0}.console.aws.amazon.com/glue/home?region={0}#database:name={1}'.format(region, database_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSBPLnLtDHg_",
        "colab_type": "text"
      },
      "source": [
        "AWS Glue ETL & Balance Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNkuxEmuDMc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "from awsglue.transforms import *\n",
        "from awsglue.utils import getResolvedOptions\n",
        "from pyspark.context import SparkContext\n",
        "from awsglue.context import GlueContext\n",
        "from awsglue.job import Job\n",
        "\n",
        "args = getResolvedOptions(sys.argv, ['JOB_NAME', 'S3_OUTPUT_BUCKET', 'S3_OUTPUT_KEY_PREFIX', 'DATABASE_NAME', 'REGION'])\n",
        "\n",
        "sc = SparkContext()\n",
        "glueContext = GlueContext(sc)\n",
        "spark = glueContext.spark_session\n",
        "job = Job(glueContext)\n",
        "job.init(args['JOB_NAME'], args)\n",
        "\n",
        "datasource0 = glueContext.create_dynamic_frame.from_catalog(database=args['DATABASE_NAME'], table_name='r_training_dbo_labelled', transformation_ctx = \"datasource0\")\n",
        "\n",
        "# datasource1 = glueContext.create_dynamic_frame.from_catalog(database=args['DATABASE_NAME'], table_name='r_training_dbo_label', transformation_ctx = \"datasource1\")\n",
        "# remap data here as needed to generate output\n",
        "# datasource2 = RenameField.apply(datasource1, \"id\", \"xyzid\")\n",
        "# datasource3 = datasource2.join( [\"abc_id\"],[\"id\"], datasource0, transformation_ctx = \"join\")\n",
        "# applymapping1 = ApplyMapping.apply(frame = datasource3, mappings = [(\"context\", \"string\", \"context\", \"string\"), (\"label\", \"string\", \"label\", \"string\")], transformation_ctx = \"applymapping1\")\n",
        "\n",
        "df1 = datasource0.toDF()\n",
        "\n",
        "df = df1\n",
        "\n",
        "per_category_count = df.groupBy('label').count().collect()\n",
        "\n",
        "#Find the category with least data\n",
        "counts = [x['count'] for x in per_category_count]\n",
        "min_count = float(min(counts))\n",
        "factors = map(lambda x: (x['label_category'], min_count/float(x['count'])), per_category_count)\n",
        "\n",
        "samples = []\n",
        "for category, n in factors:\n",
        "  sample = glueContext.create_dynamic_frame.from_catalog(database=args['DATABASE_NAME'], table_name='r_training_dbo_labelled',\n",
        "      push_down_predicate = \"label_category == '{}'\".format(category)\n",
        "  )\n",
        "  sample = sample.toDF().sample(\n",
        "      withReplacement=False,\n",
        "      fraction=n,\n",
        "      seed=42\n",
        "  )\n",
        "  samples.append(sample)\n",
        "\n",
        "balanced_df = samples[0]\n",
        "for sample in samples[1:]\n",
        "  balanced_df = balanced_df.union(sample)\n",
        "\n",
        "#Backup balanced dataset\n",
        "balanced = DynamicFrame.fromDF(balanced_df, glueContext, \"balanced\")\n",
        "\n",
        "parquet_output_path = 's3://' + os.path.join(args['S3_OUTPUT_BUCKET'], args['S3_OUTPUT_KEY_PREFIX'])\n",
        "\n",
        "sampled_data_sink = glueContext.write_dynamic_frame.from_options(\n",
        "    frame = balanced,\n",
        "    connection_type='s3',\n",
        "    connection_options={\"path\": parquet_output_path, \"partitionKeys\": [[\"label_category\"]]},\n",
        "    format=\"parguet\"\n",
        ")\n",
        "\n",
        "#Split balanced into train, test, and validation sets\n",
        "train, validation, test = balanced_df.randomSplit(weights = [.6,.2, .2], seed=42)\n",
        "\n",
        "# Repartition the data frame to store sets into single file and convert to DynamicFrame\n",
        "train_set = DynamicFrame.fromDF(train.repartition(1), glueContext, \"train\")\n",
        "validation_set = DynamicFrame.fromDF(validation.repartition(1), glueContext, \"validation\")\n",
        "test_set = DynamicFrame.fromDF(test.repartition(1), glueContext, \"test\")\n",
        "\n",
        "csv_output_path = 's3://' + os.path.join(args['S3_OUTPUT_BUCKET'], args['S3_OUTPUT_KEY_PREFIX'])\n",
        "\n",
        "train_data_sink = glueContext.write_dynamic_frame.from_options(\n",
        "    frame = train_set,\n",
        "    connection_type='s3',\n",
        "    connection_options={\"path\": \"{}/train\".format(csv_output_path)},\n",
        "    format = \"csv\",\n",
        "    format_options = {\"seperator\": \" \", \"writeHeader\": False, \"quoteChar\": \"-1\"}}\n",
        ")\n",
        "\n",
        "validation_data_sink = glueContext.write_dynamic_frame.from_options(\n",
        "    frame = validation_set,\n",
        "    connection_type='s3',\n",
        "    connection_options={\"path\": \"{}/validation\".format(csv_output_path)},\n",
        "    format = \"csv\",\n",
        "    format_options = {\"seperator\": \" \", \"writeHeader\": False, \"quoteChar\": \"-1\"}}\n",
        ")\n",
        "\n",
        "test_data_sink = glueContext.write_dynamic_frame.from_options(\n",
        "    frame = test_set,\n",
        "    connection_type='s3',\n",
        "    connection_options={\"path\": \"{}/test\".format(csv_output_path)},\n",
        "    format = \"csv\",\n",
        "    format_options = {\"seperator\": \" \", \"writeHeader\": False, \"quoteChar\": \"-1\"}}\n",
        ")\n",
        "\n",
        "job.commit()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyWpgSXaENie",
        "colab_type": "text"
      },
      "source": [
        "Upload the ETL Script to S3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uC3cM-LEQCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "script_location = sagemaker_session.upload_data(path='training_etl.py', bucket=bucket, key_prefix='codes')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXGjl6k8CT6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Output location of the data.\n",
        "s3_output_key_prefix = 'datalake/training'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3g-BQDHChVQ",
        "colab_type": "text"
      },
      "source": [
        "Creating jobs with AWS Glue"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8jPYDNkCl8a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import gmtime, strftime\n",
        "import time\n",
        "\n",
        "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
        "\n",
        "job_name = 'training-etl-' + timestamp_prefix\n",
        "response = glue.create_job(\n",
        "    Name=job_name,\n",
        "    Description='PySpark job to convert training SQL Server tables data to parquet and training, validation and test sets',\n",
        "    Role=role, # you can pass your existing AWS Glue role here if you have used Glue before\n",
        "    ExecutionProperty={\n",
        "        'MaxConcurrentRuns': 1\n",
        "    },\n",
        "    Command={\n",
        "        'Name': 'glueetl',\n",
        "        'ScriptLocation': script_location\n",
        "    },\n",
        "    DefaultArguments={\n",
        "        '--job-language': 'python',\n",
        "        '--job-bookmark-option': 'job-bookmark-disable'\n",
        "    },\n",
        "    AllocatedCapacity=5,\n",
        "    Timeout=60,\n",
        ")\n",
        "glue_job_name = response['Name']\n",
        "print(glue_job_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xD8mb6NGC0WT",
        "colab_type": "text"
      },
      "source": [
        "Run jobs with AWS Glue"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07mhWZnhC3OR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "job_run_id = glue.start_job_run(JobName=job_name,\n",
        "                                       Arguments = {\n",
        "                                        '--S3_OUTPUT_BUCKET': bucket,\n",
        "                                        '--S3_OUTPUT_KEY_PREFIX': s3_output_key_prefix,\n",
        "                                        '--DATABASE_NAME': database_name,\n",
        "                                        '--REGION': region\n",
        "                                       })['JobRunId']\n",
        "print(job_run_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUC1vPxUC50W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "job_run_status = glue.get_job_run(JobName=job_name,RunId=job_run_id)['JobRun']['JobRunState']\n",
        "while job_run_status not in ('FAILED', 'SUCCEEDED', 'STOPPED'):\n",
        "    job_run_status = glue.get_job_run(JobName=job_name,RunId=job_run_id)['JobRun']['JobRunState']\n",
        "    print (job_run_status)\n",
        "    time.sleep(60)\n",
        "print(job_run_status)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObhJq7hVKzPo",
        "colab_type": "text"
      },
      "source": [
        "Model training through AWS Sagemaker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgZxCgiSK7RX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import boto3\n",
        "import pandas\n",
        "import sagemaker\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m85GE4mEHqoW",
        "colab_type": "text"
      },
      "source": [
        "Clean Up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M139XgDuHsgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "response = cfn.delete_stack(StackName=rds_stack_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4sxo3hFIFel",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "response = glue.delete_crawler(Name=crawler_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYNXT7CAIETo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "response = glue.delete_job(JobName=glue_job_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3yHctQSIBw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "response = glue.delete_database(\n",
        "    CatalogId = account_id,\n",
        "    Name = database_name\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-FDQd8QH_ga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "response = glue.delete_connection(\n",
        "    CatalogId=account_id,\n",
        "    ConnectionName=db_connection_name\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSPmIhLRIMH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "waiter = cfn.get_waiter('stack_delete_complete')\n",
        "waiter.wait(\n",
        "    StackName=rds_stack_name\n",
        ")\n",
        "\n",
        "print('The wait is over for {0}'.format(rds_stack_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fa51WOuINVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!aws s3 rb s3://$bucket --force"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYQ3RivwIPSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not use_existing:\n",
        "    workshop.vpc_cleanup(vpc_id)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}